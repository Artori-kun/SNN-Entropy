{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c8eead1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from brian2 import *\n",
    "import os\n",
    "import random\n",
    "from entropy.entropy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0084cde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 160 training samples and 40 test samples.\n"
     ]
    }
   ],
   "source": [
    "training_data = []\n",
    "test_data = []\n",
    "\n",
    "# load train data\n",
    "train_path = \"/Users/minhhieunguyen/Documents/Projects/Dissertation/Code/spike_data/sr1000/train\"\n",
    "for filename in os.listdir(train_path):\n",
    "    if filename.endswith(\".npz\"):\n",
    "        data = np.load(os.path.join(train_path, filename))\n",
    "        spike_matrix = data['spike_matrix']\n",
    "        label = data['label'].item()\n",
    "        training_data.append((spike_matrix, label))\n",
    "\n",
    "# load test data\n",
    "test_path = \"/Users/minhhieunguyen/Documents/Projects/Dissertation/Code/spike_data/sr1000/test\"\n",
    "for filename in os.listdir(test_path):\n",
    "    if filename.endswith(\".npz\"):\n",
    "        data = np.load(os.path.join(test_path, filename))\n",
    "        spike_matrix = data['spike_matrix']\n",
    "        label = data['label'].item()\n",
    "        test_data.append((spike_matrix, label))\n",
    "\n",
    "random.shuffle(training_data)\n",
    "random.shuffle(test_data)\n",
    "\n",
    "print(f\"Loaded {len(training_data)} training samples and {len(test_data)} test samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccfb4d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def counts_from_new_spikes(mon, start_idx, n_outputs):\n",
    "    \"\"\"\n",
    "    Get spike counts for each output neuron from a spike monitor, starting from a specific index.\n",
    "    Since spike monitor store all spikes across inputs, we need to filter them based on the start index.\n",
    "\n",
    "    Args:\n",
    "        mon (SpikeMonitor): The spike monitor to get spikes from.\n",
    "        start_idx (int): The index to start counting from.\n",
    "        n_outputs (int): The number of output neurons.\n",
    "    \"\"\"\n",
    "    ii = np.array(mon.i[start_idx:])\n",
    "    if ii.size == 0:\n",
    "        return np.zeros(n_outputs, dtype=int)\n",
    "    return np.bincount(ii, minlength=n_outputs)\n",
    "\n",
    "def infer_mapping_mean(train_log_counts, n_output=2, labels_tuple=(0 , 1)):\n",
    "    \"\"\"\n",
    "    Infer the mapping from spike counts to labels based on the mean spike counts for each label.\n",
    "\n",
    "    Args:\n",
    "        train_log_counts (np.ndarray): The training log counts.\n",
    "        n_output (int): The number of output neurons.\n",
    "        labels_tuple (tuple): A tuple containing the labels for the outputs.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping each label to its corresponding output neuron index.\n",
    "    \"\"\"\n",
    "    \n",
    "    # mean spike per (label, output neuron)\n",
    "    mean = {\n",
    "        lbl: np.mean(np.vstack(train_log_counts[lbl]), axis=0) \n",
    "        if len(train_log_counts[lbl]) > 0 else np.zeros(n_output, dtype=int)\n",
    "        for lbl in labels_tuple\n",
    "    }\n",
    "    \n",
    "    # 2x2 assignment matrix\n",
    "    # rows are labels, columns are output neurons\n",
    "    M = np.vstack([mean[lbl] for lbl in labels_tuple])\n",
    "    \n",
    "    # try mapping: label0-> argmax row0, label1-> other\n",
    "    a0 = int(np.argmax(M[0])); map1 = {labels_tuple[0]: a0, labels_tuple[1]: 1 - a0}\n",
    "    score1 = M[0, map1[labels_tuple[0]]] + M[1, map1[labels_tuple[1]]]\n",
    "    \n",
    "    # try mapping label1-> argmax row1, label0-> other\n",
    "    a1 = int(np.argmax(M[1])); map2 = {labels_tuple[1]: a1, labels_tuple[0]: 1 - a1}\n",
    "    score2 = M[1, map2[labels_tuple[1]]] + M[0, map2[labels_tuple[0]]]\n",
    "    \n",
    "    chosen = map1 if score1 >= score2 else map2\n",
    "    neuron_to_label = {neuron: lbl for lbl, neuron in chosen.items()}\n",
    "    \n",
    "    return neuron_to_label, M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60e9dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_calculator = EntropyCalculator()\n",
    "def mi_matrix(output_spikes, input_spikes):\n",
    "    \"\"\"\n",
    "    Calculate the mutual information of the model.\n",
    "    Calculation includes the MI for each output neuron and the overall MI of the model.\n",
    "    \n",
    "    Formula: I(S;R) = ∑p(s,r) log(p(s,r)/(p(s)p(r)))\n",
    "    \n",
    "    where:\n",
    "        - S is the input spikes\n",
    "        - R is the output spikes\n",
    "        - p(s,r) is the joint probability of input and output spikes\n",
    "        - p(s) is the marginal probability of input spikes\n",
    "        - p(r) is the marginal probability of output spikes\n",
    "\n",
    "    Args:\n",
    "        output_spikes (list): Spikes from the output layer.\n",
    "        input_spikes (list): Spikes from the input layer.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: List of MI (neuron0, neuron1, overall).\n",
    "    \"\"\"\n",
    "    output_spikes = np.array(output_spikes)\n",
    "    input_spikes = np.array(input_spikes)\n",
    "    \n",
    "    neuron0 = 0\n",
    "    neuron1 = 0\n",
    "    model_mi = 0\n",
    "\n",
    "    for input_spike in input_spikes:\n",
    "        if not isinstance(input_spike, np.ndarray):\n",
    "            raise ValueError(\"Input spikes must be a numpy array.\")\n",
    "        \n",
    "        neuron0 += entropy_calculator.mutual_information(input_spike, output_spikes[0])\n",
    "        neuron1 += entropy_calculator.mutual_information(input_spike, output_spikes[1])\n",
    "        model_mi = neuron0 + neuron1\n",
    "\n",
    "    return neuron0, neuron1, model_mi\n",
    "\n",
    "def hc_matrix(output_spikes, input_spikes):\n",
    "    \"\"\"\n",
    "    Calculate the conditional entropy of the model.\n",
    "    Calculation includes the conditional entropy for each output neuron and the overall conditional entropy of the model.\n",
    "    \n",
    "    Formula: H(R|S) = -∑p(s)∑p(r|s) log(p(r|s))\n",
    "    \n",
    "    Args:\n",
    "        output_spikes (list): Spikes from the output layer.\n",
    "        input_spikes (list): Spikes from the input layer.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: List of entropies (neuron0, neuron1, overall).\n",
    "    \"\"\"\n",
    "    output_spikes = np.array(output_spikes)\n",
    "    input_spikes = np.array(input_spikes)\n",
    "    \n",
    "    neuron0 = 0\n",
    "    neuron1 = 0\n",
    "    model_hc = 0\n",
    "    \n",
    "    for input_spike in input_spikes:\n",
    "        if not isinstance(input_spike, np.ndarray):\n",
    "            raise ValueError(\"Input spikes must be a numpy array.\")\n",
    "        \n",
    "        neuron0 += entropy_calculator.conditional_entropy(output_spikes[0], input_spike)\n",
    "        neuron1 += entropy_calculator.conditional_entropy(output_spikes[1], input_spike)\n",
    "        model_hc = neuron0 + neuron1\n",
    "\n",
    "    return neuron0, neuron1, model_hc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4f09c02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefs.codegen.target = 'numpy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7be9a336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "w mean/min/max: 0.1125 0.10899363186605823 0.11409520472741518\n",
      "v_th: [0.38  0.379]\n",
      "Neuron to label mapping: {0: 0, 1: 1}\n",
      "Test accuracy: 0.53\n",
      "{'epoch': 1, 'mapping': {0: 0, 1: 1}, 'accuracy': 0.525, 'shannon_entropy': 0.10764026010123746, 'entropy_rate': 0.10643325550141494, 'conditional_entropy': 1.6811883371947842, 'mutual_information': 0.04105582442501558, 'neuron0': {'shannon_entropy': 0.10725700697009306, 'entropy_rate': 0.10603410220654523, 'conditional_entropy': 0.8380331995030511, 'mutual_information': 0.020022856257693643, 'spike_count': [11, 28, 14, 33, 13, 21, 17, 10, 10, 9, 1, 10, 10, 15, 16, 16, 13, 16, 8, 17, 7, 11, 12, 16, 11, 10, 16, 11, 9, 15, 7, 20, 21, 35, 11, 10, 37, 15, 12, 9]}, 'neuron1': {'shannon_entropy': 0.10802351323238188, 'entropy_rate': 0.10683240879628467, 'conditional_entropy': 0.843155137691733, 'mutual_information': 0.021032968167321957, 'spike_count': [11, 28, 14, 34, 13, 21, 17, 10, 10, 9, 1, 10, 11, 15, 16, 17, 13, 16, 8, 17, 8, 11, 13, 16, 11, 10, 16, 11, 9, 15, 7, 20, 21, 35, 11, 10, 37, 15, 12, 9]}}\n",
      "Epoch 2/5\n",
      "w mean/min/max: 0.11249999999999999 0.10928542206398813 0.11401900277431143\n",
      "v_th: [0.38  0.379]\n",
      "Neuron to label mapping: {0: 0, 1: 1}\n",
      "Test accuracy: 0.50\n",
      "{'epoch': 2, 'mapping': {0: 0, 1: 1}, 'accuracy': 0.5, 'shannon_entropy': 0.1066731850501289, 'entropy_rate': 0.10531101886764452, 'conditional_entropy': 1.6821410511997972, 'mutual_information': 0.02462990960226554, 'neuron0': {'shannon_entropy': 0.10638120056425926, 'entropy_rate': 0.10491045825156955, 'conditional_entropy': 0.8383217937869883, 'mutual_information': 0.012727810727085754, 'spike_count': [11, 28, 14, 33, 12, 20, 16, 10, 10, 9, 1, 10, 10, 15, 16, 16, 13, 16, 8, 17, 7, 11, 12, 16, 11, 10, 16, 11, 8, 15, 7, 20, 20, 34, 11, 10, 37, 15, 12, 9]}, 'neuron1': {'shannon_entropy': 0.10696516953599855, 'entropy_rate': 0.10571157948371952, 'conditional_entropy': 0.8438192574128086, 'mutual_information': 0.011902098875179784, 'spike_count': [11, 28, 14, 33, 13, 21, 17, 10, 10, 9, 1, 10, 10, 15, 16, 16, 13, 16, 8, 17, 7, 11, 12, 16, 11, 10, 16, 11, 8, 15, 7, 20, 21, 34, 11, 10, 37, 15, 12, 9]}}\n",
      "Epoch 3/5\n",
      "w mean/min/max: 0.11249999999999999 0.10903565324443065 0.11409239205958573\n",
      "v_th: [0.38  0.379]\n",
      "Neuron to label mapping: {0: 0, 1: 1}\n",
      "Test accuracy: 0.50\n",
      "{'epoch': 3, 'mapping': {0: 0, 1: 1}, 'accuracy': 0.5, 'shannon_entropy': 0.1066731850501289, 'entropy_rate': 0.10531101886764452, 'conditional_entropy': 1.6814225770194717, 'mutual_information': 0.02534838378259103, 'neuron0': {'shannon_entropy': 0.10638120056425926, 'entropy_rate': 0.10491045825156955, 'conditional_entropy': 0.8376636961057653, 'mutual_information': 0.01338590840830882, 'spike_count': [11, 28, 14, 33, 12, 20, 16, 10, 10, 9, 1, 10, 10, 15, 16, 16, 13, 16, 8, 17, 7, 11, 12, 16, 11, 10, 16, 11, 8, 15, 7, 20, 20, 34, 11, 10, 37, 15, 12, 9]}, 'neuron1': {'shannon_entropy': 0.10696516953599855, 'entropy_rate': 0.10571157948371952, 'conditional_entropy': 0.8437588809137063, 'mutual_information': 0.011962475374282208, 'spike_count': [11, 28, 14, 33, 13, 21, 17, 10, 10, 9, 1, 10, 10, 15, 16, 16, 13, 16, 8, 17, 7, 11, 12, 16, 11, 10, 16, 11, 8, 15, 7, 20, 21, 34, 11, 10, 37, 15, 12, 9]}}\n",
      "Epoch 4/5\n",
      "w mean/min/max: 0.1125 0.10904581861652551 0.11398256068849429\n",
      "v_th: [0.38  0.379]\n",
      "Neuron to label mapping: {0: 0, 1: 1}\n",
      "Test accuracy: 0.50\n",
      "{'epoch': 4, 'mapping': {0: 0, 1: 1}, 'accuracy': 0.5, 'shannon_entropy': 0.1066731850501289, 'entropy_rate': 0.10531101886764452, 'conditional_entropy': 1.6815178378328877, 'mutual_information': 0.02525312296917519, 'neuron0': {'shannon_entropy': 0.10638120056425926, 'entropy_rate': 0.10491045825156955, 'conditional_entropy': 0.8377498760453161, 'mutual_information': 0.013299728468758007, 'spike_count': [11, 28, 14, 33, 12, 20, 16, 10, 10, 9, 1, 10, 10, 15, 16, 16, 13, 16, 8, 17, 7, 11, 12, 16, 11, 10, 16, 11, 8, 15, 7, 20, 20, 34, 11, 10, 37, 15, 12, 9]}, 'neuron1': {'shannon_entropy': 0.10696516953599855, 'entropy_rate': 0.10571157948371952, 'conditional_entropy': 0.8437679617875713, 'mutual_information': 0.011953394500417183, 'spike_count': [11, 28, 14, 33, 13, 21, 17, 10, 10, 9, 1, 10, 10, 15, 16, 16, 13, 16, 8, 17, 7, 11, 12, 16, 11, 10, 16, 11, 8, 15, 7, 20, 21, 34, 11, 10, 37, 15, 12, 9]}}\n",
      "Epoch 5/5\n",
      "w mean/min/max: 0.1125 0.10927539781212489 0.11403129734614534\n",
      "v_th: [0.38  0.379]\n",
      "Neuron to label mapping: {0: 0, 1: 1}\n",
      "Test accuracy: 0.50\n",
      "{'epoch': 5, 'mapping': {0: 0, 1: 1}, 'accuracy': 0.5, 'shannon_entropy': 0.1066731850501289, 'entropy_rate': 0.10531101886764452, 'conditional_entropy': 1.681784014131034, 'mutual_information': 0.024986946671028666, 'neuron0': {'shannon_entropy': 0.10638120056425926, 'entropy_rate': 0.10491045825156955, 'conditional_entropy': 0.838177441706029, 'mutual_information': 0.01287216280804504, 'spike_count': [11, 28, 14, 33, 12, 20, 16, 10, 10, 9, 1, 10, 10, 15, 16, 16, 13, 16, 8, 17, 7, 11, 12, 16, 11, 10, 16, 11, 8, 15, 7, 20, 20, 34, 11, 10, 37, 15, 12, 9]}, 'neuron1': {'shannon_entropy': 0.10696516953599855, 'entropy_rate': 0.10571157948371952, 'conditional_entropy': 0.8436065724250048, 'mutual_information': 0.012114783862983626, 'spike_count': [11, 28, 14, 33, 13, 21, 17, 10, 10, 9, 1, 10, 10, 15, 16, 16, 13, 16, 8, 17, 7, 11, 12, 16, 11, 10, 16, 11, 8, 15, 7, 20, 21, 34, 11, 10, 37, 15, 12, 9]}}\n"
     ]
    }
   ],
   "source": [
    "defaultclock.dt = 1 * ms\n",
    "start_scope()\n",
    "\n",
    "n_input = 8\n",
    "n_output = 2\n",
    "n_epoch = 5\n",
    "sim_duration = 1000 * ms\n",
    "T = int(sim_duration / defaultclock.dt)\n",
    "\n",
    "taupre = taupost = 50 * ms\n",
    "taum = 20 * ms\n",
    "taue = 5 * ms\n",
    "gmax = 0.8\n",
    "dApre = 3e-3\n",
    "dApost = dApre\n",
    "# v_thresh = 0.2\n",
    "is_inhibitated = False\n",
    "wmin = 0.01\n",
    "\n",
    "eqs = '''\n",
    "        dv/dt = (ge - v) / taum + I_ext : 1\n",
    "        dge/dt = -ge / taue : 1\n",
    "        I_ext : Hz\n",
    "        v_th : 1\n",
    "        '''\n",
    "\n",
    "# input layer\n",
    "input_group = SpikeGeneratorGroup(\n",
    "    n_input,\n",
    "    [],\n",
    "    [] * ms\n",
    ")\n",
    "\n",
    "# output layer\n",
    "output_group = NeuronGroup(\n",
    "    n_output,\n",
    "    eqs,\n",
    "    threshold='v > v_th',\n",
    "    reset='v = 0.0',\n",
    "    refractory=5*ms,\n",
    "    method='exact'\n",
    ")\n",
    "output_group.v = 0\n",
    "output_group.v_th = 0.25\n",
    "output_group.I_ext = 0*Hz\n",
    "\n",
    "#synapse\n",
    "syn = Synapses(\n",
    "    input_group,\n",
    "    output_group,\n",
    "    '''\n",
    "    w : 1\n",
    "    plastic : 1\n",
    "    dApre/dt = -Apre / taupre : 1 (event-driven)\n",
    "    dApost/dt = -Apost / taupost : 1 (event-driven)\n",
    "    ''',\n",
    "    on_pre='''\n",
    "    ge_post += w\n",
    "    Apre += dApre\n",
    "    w = clip(w - Apost * plastic * w, wmin, gmax)\n",
    "    ''',\n",
    "    on_post='''\n",
    "    Apost += dApost\n",
    "    w = clip(w + Apre * plastic * (gmax - w), wmin, gmax)\n",
    "    '''\n",
    ")\n",
    "\n",
    "syn.connect()  # Full connectivity\n",
    "seed(42)  # For reproducibility\n",
    "syn.w = '0.1 + 0.04 * rand()'\n",
    "syn.plastic = 1\n",
    "\n",
    "output_mon = SpikeMonitor(output_group)\n",
    "input_mon = SpikeMonitor(input_group)\n",
    "net = Network(collect())\n",
    "\n",
    "logs = []\n",
    "\n",
    "def rate_homeostasis_update(counts, \n",
    "                            output_group,\n",
    "                            T, \n",
    "                            eta=0.001, \n",
    "                            r_target=50.0):\n",
    "    \n",
    "    rates = counts.astype(float)\n",
    "    delta = eta * (rates - r_target)\n",
    "\n",
    "    new_th = output_group.v_th[:] + delta\n",
    "    new_th = np.clip(new_th, 0.22, 0.38)\n",
    "    output_group.v_th = new_th\n",
    "\n",
    "def run_sample(spike_matrix,\n",
    "               net,\n",
    "               input_group,\n",
    "               output_mon,\n",
    "               n_output=2,\n",
    "               sim_duration=1000 * ms,\n",
    "               label=None,\n",
    "               bias=5*Hz,\n",
    "               t_bias=200*ms,\n",
    "               enable_plasticity=True):\n",
    "    syn.plastic = 1 if enable_plasticity else 0\n",
    "    T = int(sim_duration / defaultclock.dt)\n",
    "    t_warmup = 200*ms\n",
    "\n",
    "    # adjust spike time to match the simulation time step\n",
    "    t_start = net.t\n",
    "    t_stop = t_start + sim_duration\n",
    "    \n",
    "    # set input spikes\n",
    "    # move the spike times to the current simulation time\n",
    "    idx, tt = np.where(spike_matrix == 1)\n",
    "    if idx.size > 0:\n",
    "        times = (tt * defaultclock.dt) + t_start\n",
    "        input_group.set_spikes(idx, times)\n",
    "    else:\n",
    "        input_group.set_spikes([], []*ms)\n",
    "    \n",
    "    output_group.v = 0\n",
    "    output_group.ge = 0\n",
    "    if enable_plasticity and (label is not None):\n",
    "        output_group.I_ext = 0*Hz\n",
    "        net.run(t_warmup)\n",
    "        if label == 0:\n",
    "            output_group.I_ext = [bias, -bias]\n",
    "        else:\n",
    "            output_group.I_ext = [-bias, bias]\n",
    "        net.run(t_bias)\n",
    "        output_group.I_ext = 0*Hz\n",
    "        net.run(sim_duration - t_warmup - t_bias, report=None)\n",
    "    else:\n",
    "        output_group.I_ext = 0*Hz\n",
    "        net.run(sim_duration, report=None)\n",
    "\n",
    "    # get spike counts\n",
    "    # counts = counts_from_new_spikes(output_mon, start_idx, n_output)\n",
    "    mask = (output_mon.t >= t_start) & (output_mon.t < t_stop)\n",
    "    i_sel = np.asarray(output_mon.i[mask])\n",
    "    t_sel = output_mon.t[mask]\n",
    "    \n",
    "    counts = np.bincount(i_sel, minlength=n_output) if i_sel.size > 0 else np.zeros(n_output, dtype=int)\n",
    "    \n",
    "    out_spike = np.zeros((n_output, T), dtype=int)\n",
    "    # new_i = np.array(output_mon.i[start_idx:])\n",
    "    # new_t = np.array(output_mon.t[start_idx:])\n",
    "    if i_sel.size > 0:\n",
    "        bins = np.floor((t_sel - t_start) / defaultclock.dt).astype(int)\n",
    "        bins = np.clip(bins, 0, T - 1)\n",
    "        out_spike[i_sel, bins] = 1\n",
    "    return counts, out_spike\n",
    "\n",
    "def normalize_incoming(syn,\n",
    "                       n_output=2,\n",
    "                       w_target=1.2):\n",
    "    j_all = syn.j[:]\n",
    "    w_all = syn.w[:]\n",
    "    for j in range(n_output):\n",
    "        idx = np.where(j_all == j)[0]\n",
    "        if idx.size == 0:\n",
    "            continue\n",
    "\n",
    "        s = float(np.sum(w_all[idx]))\n",
    "        if s > 1e-12:\n",
    "            syn.w[idx] = w_all[idx] * w_target / s\n",
    "\n",
    "bias0 = 30*Hz\n",
    "bias_decay = 0.9\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    print(f'Epoch {epoch + 1}/{n_epoch}')\n",
    "    log = {}\n",
    "    log['epoch'] = epoch + 1\n",
    "    \n",
    "    if epoch >= 1:\n",
    "        # lateral inhibition\n",
    "        if not is_inhibitated:\n",
    "            inh_w = 0.02\n",
    "            inh = Synapses(\n",
    "                output_group,\n",
    "                output_group,\n",
    "                on_pre='ge_post -= inh_w'\n",
    "            )\n",
    "            inh.connect(condition='j != i')  # Full connectivity\n",
    "            net.add(inh)\n",
    "            is_inhibitated = True\n",
    "\n",
    "    # Training\n",
    "    train_log_counts = {0: [], 1: []}\n",
    "    curr_bias = bias0 * (bias_decay ** epoch)\n",
    "    for spike_matrix, label in training_data:\n",
    "        counts, _ = run_sample(spike_matrix,\n",
    "                               net=net,\n",
    "                               input_group=input_group,\n",
    "                               output_mon=output_mon,\n",
    "                               n_output=n_output,\n",
    "                               sim_duration=sim_duration,\n",
    "                               label=label,\n",
    "                               bias=curr_bias,\n",
    "                               t_bias=150*ms,\n",
    "                               enable_plasticity=True)\n",
    "        train_log_counts[label].append(counts)\n",
    "        rate_homeostasis_update(\n",
    "            counts,\n",
    "            output_group,\n",
    "            T\n",
    "        )\n",
    "    normalize_incoming(syn, n_output=n_output, w_target=0.9)\n",
    "\n",
    "    # log['weights'] = syn.w\n",
    "    print(\"w mean/min/max:\",\n",
    "      float(np.mean(syn.w[:])),\n",
    "      float(np.min(syn.w[:])),\n",
    "      float(np.max(syn.w[:])))\n",
    "    print(\"v_th:\", output_group.v_th[:])\n",
    "\n",
    "    # deprive mapping neuron-to-label\n",
    "    neuron_to_label, M = infer_mapping_mean(train_log_counts, n_output=n_output, labels_tuple=(0, 1))\n",
    "    log['mapping'] = neuron_to_label.copy()\n",
    "\n",
    "    # print(f\"Train log: {train_log_counts}\")\n",
    "    # print(f\"Mean spike (rows= labels, columns=output neurons):\\n{M}\")\n",
    "    print(f\"Neuron to label mapping: {neuron_to_label}\")\n",
    "    \n",
    "    # Testing\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    neuron0_mi = 0\n",
    "    neuron1_mi = 0\n",
    "    neuron0_hc = 0\n",
    "    neuron1_hc = 0\n",
    "    neuron0_entropy = 0\n",
    "    neuron1_entropy = 0\n",
    "    neuron0_entropy_rate = 0\n",
    "    neuron1_entropy_rate = 0\n",
    "    model_mi = 0\n",
    "    model_hc = 0\n",
    "    model_entropy = 0\n",
    "    model_entropy_rate = 0\n",
    "    \n",
    "    neuron0_spikecount = []\n",
    "    neuron1_spikecount = []\n",
    "\n",
    "    for spikes, label in test_data:\n",
    "        counts, out_spike = run_sample(spikes,\n",
    "                                       net=net,\n",
    "                                       input_group=input_group,\n",
    "                                       output_mon=output_mon,\n",
    "                                       n_output=n_output,\n",
    "                                       sim_duration=sim_duration,\n",
    "                                       enable_plasticity=False)\n",
    "\n",
    "        pred_neuron = int(np.argmax(counts))\n",
    "        pred_label = neuron_to_label.get(pred_neuron, 0)\n",
    "        correct += int(pred_label == label)\n",
    "        total += 1\n",
    "        \n",
    "        neuron0_spikecount.append(counts[0])\n",
    "        neuron1_spikecount.append(counts[1])\n",
    "        \n",
    "        # print(out_spike.shape)\n",
    "        # print(spikes.shape)\n",
    "\n",
    "        neuron0_entropy += entropy_calculator.shannon_entropy(out_spike[0])\n",
    "        neuron1_entropy += entropy_calculator.shannon_entropy(out_spike[1])\n",
    "        model_entropy += entropy_calculator.shannon_entropy(out_spike)\n",
    "        \n",
    "        neuron0_entropy_rate += entropy_calculator.entropy_rate(out_spike[0])\n",
    "        neuron1_entropy_rate += entropy_calculator.entropy_rate(out_spike[1])\n",
    "        model_entropy_rate += entropy_calculator.entropy_rate(out_spike)\n",
    "\n",
    "        _neuron0_hc, _neuron1_hc, _model_hc = hc_matrix(out_spike, spikes)\n",
    "        neuron0_hc += _neuron0_hc\n",
    "        neuron1_hc += _neuron1_hc\n",
    "        model_hc += _model_hc\n",
    "        \n",
    "        _neuron0_mi, _neuron1_mi, _model_mi = mi_matrix(out_spike, spikes)\n",
    "        neuron0_mi += _neuron0_mi\n",
    "        neuron1_mi += _neuron1_mi\n",
    "        model_mi += _model_mi\n",
    "\n",
    "    accuracy = correct / max(total, 1)\n",
    "    log['accuracy'] = accuracy\n",
    "    print(f\"Test accuracy: {accuracy:.2f}\")\n",
    "    \n",
    "    log['shannon_entropy'] = model_entropy / len(test_data)\n",
    "    log['entropy_rate'] = model_entropy_rate / len(test_data)\n",
    "    log['conditional_entropy'] = model_hc / len(test_data)\n",
    "    log['mutual_information'] = model_mi / len(test_data)\n",
    "    \n",
    "    log['neuron0'] = {\n",
    "        'shannon_entropy': neuron0_entropy / len(test_data),\n",
    "        'entropy_rate': neuron0_entropy_rate / len(test_data),\n",
    "        'conditional_entropy': neuron0_hc / len(test_data),\n",
    "        'mutual_information': neuron0_mi / len(test_data),\n",
    "        'spike_count': neuron0_spikecount\n",
    "    }\n",
    "    \n",
    "    log['neuron1'] = {\n",
    "        'shannon_entropy': neuron1_entropy / len(test_data),\n",
    "        'entropy_rate': neuron1_entropy_rate / len(test_data),\n",
    "        'conditional_entropy': neuron1_hc / len(test_data),\n",
    "        'mutual_information': neuron1_mi / len(test_data),\n",
    "        'spike_count': neuron1_spikecount\n",
    "    }\n",
    "    \n",
    "    print(log)\n",
    "    \n",
    "    logs.append(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db028429",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
